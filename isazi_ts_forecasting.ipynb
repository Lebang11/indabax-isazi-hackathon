{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting with GluonTS\n",
    "\n",
    "This notebook demonstrates a simple time series forecasting approach using the GluonTS library with MXNet backend. The process includes data preparation, model training, prediction, and error computation. We will use a dataset containing sales data to forecast future sales volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: gluonts[mxnet]\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install pandas gluonts[mxnet] orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.mx import SimpleFeedForwardEstimator, Trainer\n",
    "from gluonts.evaluation import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "First, we load and prepare our dataset, splitting it into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/isazi_ts_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(data_path, parse_dates\u001b[38;5;241m=\u001b[39m[date_col])\n\u001b[1;32m     11\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset paths and column definitions\n",
    "time_series_id_col = 'product_code'\n",
    "date_col = 'sales_date'\n",
    "target_col = 'volume'\n",
    "freq = \"1D\"\n",
    "prediction_length = 28\n",
    "data_path = \"data/isazi_ts_dataset.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(data_path, parse_dates=[date_col])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time_series(df, prediction_length):\n",
    "    \"\"\"\n",
    "    Function to split off a df of time series into time series, where the second time series\n",
    "    includes the last `prediction_length` time steps. \n",
    "    \"\"\"\n",
    "    # Create an empty dataframe for train and validation sets\n",
    "    train_df = pd.DataFrame()\n",
    "    validation_df = pd.DataFrame()\n",
    "    \n",
    "    # Group by the time series identifier\n",
    "    grouped = df.groupby(time_series_id_col)\n",
    "    \n",
    "    # Iterate over each group (i.e., each individual time series)\n",
    "    for item_id, group in grouped:\n",
    "        # Sort the group by date if it's not already sorted\n",
    "        group = group.sort_index()\n",
    "        \n",
    "        # Define the split point\n",
    "        split_point = len(group) - prediction_length\n",
    "        \n",
    "        # Split the data into training and validation sets\n",
    "        train_group = group.iloc[:split_point]\n",
    "        validation_group = group\n",
    "        \n",
    "        # Append to the respective dataframes\n",
    "        train_df = pd.concat([train_df, train_group])\n",
    "        validation_df = pd.concat([validation_df, validation_group])\n",
    "    \n",
    "    return train_df, validation_df\n",
    "    \n",
    "train_df, test_df = split_time_series(df, prediction_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "We will train a simple feedforward neural network to predict future sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 100/100 [00:07<00:00, 13.96it/s, epoch=1/10, avg_epoch_loss=5.84]\n",
      "13it [00:00, 15.68it/s, epoch=1/10, validation_avg_epoch_loss=5.37]\n",
      "100%|████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.52it/s, epoch=2/10, avg_epoch_loss=5.38]\n",
      "13it [00:00, 16.48it/s, epoch=2/10, validation_avg_epoch_loss=5.32]\n",
      "100%|████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.81it/s, epoch=3/10, avg_epoch_loss=5.38]\n",
      "13it [00:00, 17.87it/s, epoch=3/10, validation_avg_epoch_loss=5.23]\n",
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.93it/s, epoch=4/10, avg_epoch_loss=5.3]\n",
      "13it [00:00, 16.24it/s, epoch=4/10, validation_avg_epoch_loss=5.26]\n",
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.61it/s, epoch=5/10, avg_epoch_loss=5.2]\n",
      "13it [00:00, 16.32it/s, epoch=5/10, validation_avg_epoch_loss=5.23]\n",
      "100%|████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.49it/s, epoch=6/10, avg_epoch_loss=5.21]\n",
      "13it [00:00, 17.11it/s, epoch=6/10, validation_avg_epoch_loss=5.22]\n",
      "100%|████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.34it/s, epoch=7/10, avg_epoch_loss=5.16]\n",
      "13it [00:00, 17.57it/s, epoch=7/10, validation_avg_epoch_loss=5.18]\n",
      "100%|████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.31it/s, epoch=8/10, avg_epoch_loss=5.18]\n",
      "13it [00:00, 17.77it/s, epoch=8/10, validation_avg_epoch_loss=5.17]\n",
      "100%|████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.50it/s, epoch=9/10, avg_epoch_loss=5.14]\n",
      "13it [00:00, 17.55it/s, epoch=9/10, validation_avg_epoch_loss=5.18]\n",
      "100%|███████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.71it/s, epoch=10/10, avg_epoch_loss=5.12]\n",
      "13it [00:00, 16.30it/s, epoch=10/10, validation_avg_epoch_loss=5.15]\n"
     ]
    }
   ],
   "source": [
    "def train_predictor(df):\n",
    "    \"\"\"\n",
    "    A very basic feed forward predictor using GluonTS with a training and validation set.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.set_index(date_col, inplace=True) # GluonTS wants the timestamp to be the index\n",
    "\n",
    "    # Split the training data into training and validation sets\n",
    "    train_df, val_df = split_time_series(df, prediction_length)\n",
    "    # Create the Pandas datasets\n",
    "    train_ds = PandasDataset.from_long_dataframe(train_df,\n",
    "                                                target=target_col,\n",
    "                                                item_id=time_series_id_col,\n",
    "                                                freq=freq)\n",
    "\n",
    "    val_ds = PandasDataset.from_long_dataframe(val_df,\n",
    "                                                target=target_col,\n",
    "                                                item_id=time_series_id_col,\n",
    "                                                freq=freq)\n",
    "\n",
    "    # Train a feed forward estimator\n",
    "    estimator = SimpleFeedForwardEstimator(\n",
    "        num_hidden_dimensions=[10],\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=100,\n",
    "        trainer=Trainer(ctx=\"cpu\", epochs=10, learning_rate=1e-3, num_batches_per_epoch=100),\n",
    "    )\n",
    "    predictor = estimator.train(training_data=train_ds, validation_data=val_ds)\n",
    "    return predictor\n",
    "    \n",
    "predictor = train_predictor(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Using the trained model to make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(predictor, df):\n",
    "    \"\"\"\n",
    "    Make predictions with a GluonTS predictor and return them as a df.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.set_index(date_col, inplace=True) # GluonTS wants the timestamp to be the index\n",
    "    dataset = PandasDataset.from_long_dataframe(df,\n",
    "                                                target=target_col,\n",
    "                                                item_id=time_series_id_col,\n",
    "                                                freq=freq)\n",
    "        \n",
    "    forecast_it, _ = make_evaluation_predictions(\n",
    "        dataset=dataset,\n",
    "        predictor=predictor,\n",
    "        num_samples=100,  # number of sample paths we want for evaluation\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "\n",
    "    # Initialize a list to hold all the records before converting to a DataFrame\n",
    "    records = []\n",
    "    for forecast in forecasts:\n",
    "        item_id = forecast.item_id\n",
    "        start_timestamp = forecast.start_date.start_time\n",
    "\n",
    "        # GluonTS does probabilistic forecasting with a number of samples\n",
    "        # Calculate mean targets across all samples for each date position\n",
    "        mean_targets = forecast.samples.mean(axis=0)\n",
    "        for i, target in enumerate(mean_targets):\n",
    "            # Calculate what the timestamp should be for each predicted target\n",
    "            timestamp = start_timestamp + i * pd.to_timedelta(freq)\n",
    "            # Store the prediction as a record\n",
    "            records.append({date_col: timestamp, time_series_id_col: item_id, target_col: target})\n",
    "\n",
    "    # Convert the predictions from a list of records into a DataFrame\n",
    "    preds_df = pd.DataFrame(records)\n",
    "    preds_df.set_index(date_col, inplace=True)\n",
    "    return preds_df\n",
    "\n",
    "output_preds_path = \"my_predictions.csv\"\n",
    "preds_df = make_predictions(predictor, test_df)\n",
    "preds_df.to_csv(output_preds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Calculation\n",
    "\n",
    "We compute the error between the forecasts and the actual sales to evaluate our model's performance. For this we main use overall relative error bias, the metric for the hackathon should probably be a weighted combination like `(1 - rE - 0.5*rB)*100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Report:\n",
      "A     1.199319e+06\n",
      "P     1.340588e+06\n",
      "E     5.480017e+05\n",
      "rE    4.569274e-01\n",
      "rB    1.177912e-01\n",
      "dtype: float64\n",
      "Forecast Accuracy is: 54.31%\n",
      "Forecast Bias is: 11.78%\n",
      "Bias-weighted Accuracy: 48.42%\n"
     ]
    }
   ],
   "source": [
    "def compute_error(actuals_path, preds_path):\n",
    "    \"\"\"\n",
    "    Computes the relative error and relative bias from csv files of predictions and actual targets.\n",
    "    \"\"\"\n",
    "    # Read predicted and actuals from their respective files if not already provided\n",
    "    actuals_df = pd.read_csv(actuals_path, parse_dates=[date_col])\n",
    "    preds_df = pd.read_csv(preds_path, parse_dates=[date_col])\n",
    "    \n",
    "    # Rename 'target' column to add suffix for 'preds' and 'actuals' respectively\n",
    "    actuals_df.rename(columns={target_col: f'{target_col}_actuals'}, inplace=True)\n",
    "    preds_df.rename(columns={target_col: f'{target_col}_preds'}, inplace=True)\n",
    "    \n",
    "    # Merge the two dataframes on the timestamp column and the time series identifier column\n",
    "    df = pd.merge(actuals_df, preds_df, on=[date_col, time_series_id_col])    \n",
    "    \n",
    "    actual_var = target_col + '_actuals'\n",
    "    pred_var = target_col +'_preds'\n",
    "    measure_level = [time_series_id_col, date_col]\n",
    "\n",
    "    # Drop all unecessary columns\n",
    "    keep_vars = list(set(measure_level + [actual_var, pred_var]))\n",
    "    df_filtered = df.dropna(subset=[actual_var])[keep_vars]\n",
    "    df_filtered.rename(columns={actual_var: 'A', pred_var: 'P'}, inplace=True)\n",
    "\n",
    "    # Group by measure_level and aggregate A and P\n",
    "    grouped = df_filtered.groupby(measure_level, observed=False).agg(A=('A', 'sum'), P=('P', 'sum'))\n",
    "\n",
    "    # Calculate the errors initially at measure_level (not the absolute sum yet)\n",
    "    grouped['E'] = (grouped['A'] - grouped['P']).abs()\n",
    "\n",
    "    # Aggregate all data to one row\n",
    "    grouped = grouped.sum()\n",
    "\n",
    "    # Calculate relative error (rE) and relative bias (rB)\n",
    "    grouped['rE'] = grouped['E'] / grouped['A']\n",
    "    grouped['rB'] = (grouped['P'] - grouped['A']) / grouped['A']\n",
    "    return grouped\n",
    "\n",
    "output_preds_path = \"my_predictions.csv\"\n",
    "report = compute_error(data_path, output_preds_path)\n",
    "print(\"Error Report:\")\n",
    "print(report)\n",
    "\n",
    "forecast_acc = (1 - (report.rE)) * 100\n",
    "forecast_bias = report.rB * 100\n",
    "acc_bias = forecast_acc - 0.5 * forecast_bias\n",
    "print(f\"Forecast Accuracy is: {forecast_acc:.2f}%\")\n",
    "print(f\"Forecast Bias is: {forecast_bias:.2f}%\")\n",
    "print(f\"Bias-weighted Accuracy: {acc_bias:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
